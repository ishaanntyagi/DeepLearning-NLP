{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208f553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Gpt2 Transformer now \n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2ec4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db970ded43704403bbb6df8e30ff4f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaan.narayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ishaan.narayan\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ishaan.narayan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc73dde79414a03a675369b47208f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e8955b9a5d484b9a86a1535b4974b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9378c0619a4d3fb089290fe236ce1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98267d5975a947b9a14330798da5f67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bc7f5546c94a7abbd53cc7139f4820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d124777e564867bc6101e998b0039c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb99aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what happened after the sunset?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8e73098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "output = generator(prompt, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c74706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'what happened after the sunset?\\n\\nThe answer is yes. The people had to come out of their homes. Before sunrise, the sky was so bright and clear, it was impossible to see through the windows. A large group of people came out of their homes, and at sunrise, they had to go outside, and the sky was so bright that you could see it through the windows. The people came out of their homes for the first time and they were afraid. They thought, \"I can\\'t see anything. I can\\'t see anything. I can\\'t hear anything. I can\\'t even see the moon.\" So there was fear. The people were scared.\\n\\nThe people came out of their homes, and they were afraid. They thought, \"I can\\'t see anything. I can\\'t see anything. I can\\'t hear anything. I can\\'t even hear the moon.\" So there was fear.\\n\\nThe people came out of their homes, and they were fearful. They thought, \"I can\\'t see anything. I can\\'t see anything. I can\\'t hear anything. I can\\'t even hear the moon.\" So there was fear.\\n\\nThe people came out of their homes, and they were afraid. They thought, \"I can\\'t see anything. I can'}\n"
     ]
    }
   ],
   "source": [
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3adc70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "            TEXT GENERATION\n",
      "\n",
      "\n",
      "Prompt:\n",
      "-------\n",
      "what happened after the sunset?\n",
      "\n",
      "Generated Text:\n",
      "---------------\n",
      "[{'generated_text': 'what happened after the sunset?\\n\\nThe answer is yes. The people had to come out of their homes. Before sunrise, the sky was so bright and clear, it was impossible to see through the windows. A large group of people came out of their homes, and at sunrise, they had to go outside, and the sky was so bright that you could see it through the windows. The people came out of their homes for the first time and they were afraid. They thought, \"I can\\'t see anything. I can\\'t see anything. I can\\'t hear anything. I can\\'t even see the moon.\" So there was fear. The people were scared.\\n\\nThe people came out of their homes, and they were afraid. They thought, \"I can\\'t see anything. I can\\'t see anything. I can\\'t hear anything. I can\\'t even hear the moon.\" So there was fear.\\n\\nThe people came out of their homes, and they were fearful. They thought, \"I can\\'t see anything. I can\\'t see anything. I can\\'t hear anything. I can\\'t even hear the moon.\" So there was fear.\\n\\nThe people came out of their homes, and they were afraid. They thought, \"I can\\'t see anything. I can'}]\n",
      "\n",
      "========================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = f\"\"\"\n",
    "\n",
    "\n",
    "            TEXT GENERATION\n",
    "\n",
    "\n",
    "Prompt:\n",
    "-------\n",
    "{prompt}\n",
    "\n",
    "Generated Text:\n",
    "---------------\n",
    "{output}\n",
    "\n",
    "========================================\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Step 5: Print the formatted result\n",
    "print(template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
